# marcle.ai — required environment variables
# Copy to .env and fill in values. All are optional; unconfigured services report "unknown" status.

# Global
REQUEST_TIMEOUT_SECONDS=4
CHECK_TIMEOUT_SECONDS=4
REFRESH_INTERVAL_SECONDS=30
MAX_CONCURRENCY=10
TZ=UTC
SERVICES_CONFIG_PATH=/data/services.json
OBSERVATIONS_PATH=/data/observations.json
OBSERVATIONS_HISTORY_LIMIT=200
AUDIT_LOG_PATH=/data/audit.log
AUDIT_LOG_MAX_BYTES=5242880
EXPOSE_SERVICE_URLS=false
CORS_ORIGINS=
FLAP_WINDOW_SECONDS=600
FLAP_THRESHOLD=3
# Set a strong random token before enabling admin endpoints.
ADMIN_TOKEN=

# Proxmox
PROXMOX_URL=
PROXMOX_API_TOKEN=

# UniFi
UNIFI_URL=
UNIFI_PROTECT_URL=
UNIFI_API_KEY=

# Home Assistant
HOMEASSISTANT_URL=
HOMEASSISTANT_TOKEN=

# Plex
PLEX_URL=
PLEX_TOKEN=

# Overseerr
OVERSEERR_URL=
OVERSEERR_API_KEY=

# Tautulli
TAUTULLI_URL=
TAUTULLI_API_KEY=

# Arr Stack
RADARR_URL=
RADARR_API_KEY=
SONARR_URL=
SONARR_API_KEY=

# Ollama
OLLAMA_URL=

# n8n status check target (optional)
N8N_URL=

# Self-hosted n8n runtime
# Keep n8n.marcle.ai behind Cloudflare Access (human UI/editor login).
# Do NOT protect hooks.marcle.ai with Cloudflare Access browser login, or
# external webhook deliveries will fail.
N8N_PROTOCOL=https
N8N_HOST=n8n.marcle.ai
N8N_EDITOR_BASE_URL=https://n8n.marcle.ai/
N8N_WEBHOOK_URL=https://hooks.marcle.ai/
# Required strong random value. Generate with: openssl rand -hex 32
N8N_ENCRYPTION_KEY=
N8N_DB_NAME=n8n
N8N_DB_USER=n8n
# Strong random DB password. Generate with: openssl rand -hex 24
N8N_DB_PASSWORD=

# ============================================================
# Ask App (ask.marcle.ai)
# ============================================================

# Google OAuth2
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=
# Optional explicit callback URL. If empty, computed from BASE_PUBLIC_URL.
GOOGLE_REDIRECT_URL=

# Session secret — generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
SESSION_SECRET=change-me-in-production

# Discord webhook for question notifications (legacy fallback)
DISCORD_WEBHOOK_URL=
# Discord bot token (question posting, thread creation, answer ingestion, LLM answer posting)
DISCORD_BOT_TOKEN=
# Ask intake channel where bot posts new questions
DISCORD_ASK_CHANNEL_ID=
# Discord guild/server id (used for metadata and links)
DISCORD_GUILD_ID=
# Only this role can provide accepted human answers in Discord threads
DISCORD_SUPPORT_ROLE_ID=

# SMTP (for sending answer emails)
SMTP_HOST=
SMTP_PORT=587
# Auth mailbox username/login (for example iCloud mailbox).
SMTP_USER=
SMTP_PASS=
# Sender address shown in recipient inbox (supports custom domain alias).
SMTP_FROM=
SMTP_USE_TLS=true

# Shared secret for the answer webhook endpoint
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
ASK_ANSWER_WEBHOOK_SECRET=

# Separate token for n8n Discord -> Ask integration endpoints
# Sent via header: X-N8N-TOKEN
N8N_TOKEN=

# Ask staged fallback timing (seconds)
ASK_HUMAN_WAIT_SECONDS=300
ASK_OPENAI_WAIT_SECONDS=600
ASK_WEBHOOK_MAX_BYTES=65536
ASK_SSE_MAX_CONN_PER_SESSION=2
ASK_SSE_MAX_CONN_PER_IP=10
ASK_SSE_CONN_RATE_PER_MIN=10

# Local Docker Model Runner (OpenAI-compatible) fallback
LOCAL_LLM_BASE_URL=http://172.16.2.220:12434/engines/v1
LOCAL_LLM_API_KEY=not-needed
LOCAL_LLM_MODEL=ai/llama3.2:latest
LOCAL_LLM_TIMEOUT_SECONDS=90

# OpenAI stage-3 fallback config
LLM_BASE_URL=https://api.openai.com/v1
LLM_API_KEY=
LLM_MODEL=gpt-4o-mini
OPENAI_TIMEOUT_SECONDS=600

# Points
ASK_POINTS_ENABLED=false
DEFAULT_STARTING_POINTS=10
POINTS_PER_QUESTION=1

# Public-facing base URL (used in OAuth redirects, Ask post-login redirect, Discord messages, emails)
# Example production value: https://marcle.ai
BASE_PUBLIC_URL=

# SQLite database path (persisted via Docker volume)
ASK_DB_PATH=/data/ask.db
